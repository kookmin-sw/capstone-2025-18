from paddleocr import PaddleOCR
import cv2
import numpy as np
import random
from sklearn.cluster import KMeans

# OCR ì„¤ì • ê°•í™”
ocr = PaddleOCR(
    use_angle_cls=True,
    lang='korean',  # ë˜ëŠ” 'en'ë„ ê³ ë ¤ ê°€ëŠ¥
    rec_algorithm='SVTR_LCNet',  # ë” ì •êµí•œ ì¸ì‹
    det_db_box_thresh=0.5,
    drop_score=0.3,
    show_log=False
)

def preprocess_roi(roi):
    # í™•ëŒ€ + ëŒ€ë¹„ ì¦ê°€ + ì„ ëª…í•˜ê²Œ
    roi = cv2.resize(roi, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_LINEAR)
    lab = cv2.cvtColor(roi, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(4,4))
    cl = clahe.apply(l)
    merged = cv2.merge((cl,a,b))
    roi_enhanced = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)
    return roi_enhanced

def extract_texts(cropped_img):
    result = ocr.ocr(cropped_img, cls=True)
    if not result or not result[0]:
        return []

    return [
        (text, round(conf, 2))
        for _, (text, conf) in result[0]
        if conf > 0.6  # ì‚´ì§ ì™„í™”
    ]

# ì¤‘ë³µ ì œê±° í•¨ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
def remove_duplicate_boxes(boxes, iou_threshold=0.95):
    def iou(box1, box2):
        x1, y1, x2, y2 = box1
        x1_, y1_, x2_, y2_ = box2
        inter_x1 = max(x1, x1_)
        inter_y1 = max(y1, y1_)
        inter_x2 = min(x2, x2_)
        inter_y2 = min(y2, y2_)
        inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
        box1_area = (x2 - x1) * (y2 - y1)
        box2_area = (x2_ - x1_) * (y2_ - y1_)
        union_area = box1_area + box2_area - inter_area
        return inter_area / union_area if union_area else 0
    kept = []
    for box in boxes:
        if all(iou(box, k) < iou_threshold for k in kept):
            kept.append(box)
    return kept

def expand_box_for_ocr(x1, y1, x2, y2, img_shape, margin=10):
    h, w = img_shape[:2]
    x1e = max(0, x1 - margin)
    y1e = max(0, y1 - margin)
    x2e = min(w, x2 + margin)
    y2e = min(h, y2 + margin)
    return x1e, y1e, x2e, y2e

def extract_title_line_text(result, box_height, top_ratio=0.4, height_threshold=0.8):
    if not result or not result[0]:
        return ""

    lines = []
    for line in result[0]:
        box, (text, conf) = line
        top = min(p[1] for p in box)
        bottom = max(p[1] for p in box)
        height = bottom - top
        lines.append({
            "text": text,
            "conf": conf,
            "top": top,
            "height": height
        })

    # ìƒë‹¨ 40% ë‚´ì˜ í…ìŠ¤íŠ¸
    top_lines = [l for l in lines if l["top"] < box_height * top_ratio]
    if not top_lines:
        return ""

    top_lines.sort(key=lambda l: l["top"])
    base_height = top_lines[0]["height"]

    # ì œëª©ê³¼ ìœ ì‚¬í•œ ë†’ì´ë§Œ ì¶”ì¶œ
    title_lines = [
        l for l in top_lines
        if abs(l["height"] - base_height) / base_height < (1 - height_threshold)
    ]

    # ì¥ì†Œë‚˜ ê°•ì˜ì‹¤ ì œì™¸ (ê°„ë‹¨ í‚¤ì›Œë“œ í•„í„°ë§)
    exclude_keywords = ["í˜¸ì‹¤", "ê´€", "ì¸µ", "ì‹¤", "room", "Room"]
    filtered = [
        l["text"] for l in title_lines
        if not any(keyword in l["text"] for keyword in exclude_keywords)
    ]

    return " ".join(filtered)


# ì´ë¯¸ì§€ ë¡œë“œ ë° ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§
img_path = "sample_img/sample4.png"
img = cv2.imread(img_path)
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)
h, w = img.shape[:2]
flat_img = img_lab.reshape((-1, 3))

kmeans = KMeans(n_clusters=15, random_state=42, n_init=10)
labels = kmeans.fit_predict(flat_img)
clustered = kmeans.cluster_centers_[labels].reshape((h, w, 3)).astype(np.uint8)

boxes = []
for label in np.unique(labels):
    mask = (labels.reshape((h, w)) == label).astype(np.uint8) * 255
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    for cnt in contours:
        x, y, bw, bh = cv2.boundingRect(cnt)
        area = bw * bh
        if 60 < bw < 300 and 60 < bh < 800 and area > 2000:
            roi = img[y:y+bh, x:x+bw]
            roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            _, roi_bin = cv2.threshold(roi_gray, 200, 255, cv2.THRESH_BINARY_INV)
            if cv2.countNonZero(roi_bin) > 80:
                boxes.append((x, y, x + bw, y + bh))

boxes = remove_duplicate_boxes(boxes)

# ì‹œê°í™” ë° OCR ì ìš©
output_img = img.copy()
print(f"\nğŸ“¦ ìµœì¢… ë°•ìŠ¤ ê°œìˆ˜: {len(boxes)}")
for i, (x1, y1, x2, y2) in enumerate(sorted(boxes, key=lambda b: (b[1], b[0])), 1):
    # ğŸ¨ ëœë¤ ìƒ‰ìƒ ìƒì„±
    color = tuple(random.randint(0, 255) for _ in range(3))

    # ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì›ë˜ ì¢Œí‘œ ê¸°ì¤€)
    cv2.rectangle(output_img, (x1, y1), (x2, y2), color, 2)

    # ğŸ“Œ ë§ˆì§„ ì£¼ëŠ” ê±´ OCRì—ë§Œ ì ìš©
    x1e, y1e, x2e, y2e = expand_box_for_ocr(x1, y1, x2, y2, img.shape, margin=10)
    roi = img[y1e:y2e, x1e:x2e]

    # ğŸ“Œ ì „ì²˜ë¦¬
    roi_processed = preprocess_roi(roi)

    result = ocr.ocr(roi_processed, cls=True)
    titles = extract_title_line_text(result, y2e - y1e)

    print(f"\nğŸ“¦ Box {i} ({x1}, {y1}, {x2}, {y2})")
    print(f"  ğŸŸ© [title] {titles}")

# ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
cv2.imwrite("test_result/sample4_boxed.png", output_img)
